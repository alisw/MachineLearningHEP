{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parallelism in python with Pool \n",
    "\n",
    "The pool class provides a very simple and convenient approach for simple parallelism in Python. There are four methods that are particularly interesting:\n",
    "\n",
    "- Pool.apply\n",
    "- Pool.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine_learning_hep.listfiles import list_files_dir_lev2, list_files_lev2\n",
    "from machine_learning_hep.doskimming import skimall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"default_complete.yaml\", 'r') as run_config: \n",
    "    data_config = yaml.load(run_config) \n",
    "with open(\"data/database_ml_parameters.yml\", 'r') as param_config: \n",
    "    data_param = yaml.load(param_config) \n",
    "with open(\"data/config_model_parameters.yml\", 'r') as mod_config: \n",
    "    data_model = yaml.load(mod_config) \n",
    "mcordata = \"data\"\n",
    "indexp = 0\n",
    "case = data_config[\"case\"]\n",
    "param_case = data_param[case]\n",
    "\n",
    "\n",
    "def list_create_dir(inputdir, outputdir, namea, nameb, namec, nameaout, namebout, namecout, maxfiles):\n",
    "    lista, listaout = list_files_dir_lev2(inputdir, outputdir, namea, nameaout)\n",
    "    listb, listbout = list_files_dir_lev2(inputdir, outputdir, nameb, namebout)\n",
    "    listc, listcout = list_files_dir_lev2(inputdir, outputdir, namec, namecout)\n",
    "\n",
    "    if maxfiles is not -1:\n",
    "        lista = lista[:maxfiles]\n",
    "        listb = listb[:maxfiles]\n",
    "        listc = listc[:maxfiles]\n",
    "        listaout = listaout[:maxfiles]\n",
    "        listbout = listbout[:maxfiles]\n",
    "        listcout = listcout[:maxfiles]\n",
    "\n",
    "        return lista, listb, listc, listaout, listbout, listcout\n",
    "\n",
    "def createchunks(listin, listout, maxperchunk):\n",
    "    chunks = [listin[x:x+maxperchunk]  for x in range(0, len(listin), maxperchunk)]\n",
    "    chunksout = [listout[x:x+maxperchunk] for x in range(0, len(listout), maxperchunk)]\n",
    "    return chunks, chunksout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "namefile_reco = data_param[case][\"files_names\"][\"namefile_reco\"]\n",
    "namefile_evt = data_param[case][\"files_names\"][\"namefile_evt\"]\n",
    "namefile_gen = data_param[case][\"files_names\"][\"namefile_gen\"]\n",
    "namefile_reco_skim = data_param[case][\"files_names\"][\"namefile_reco_skim\"]\n",
    "namefile_evt_skim = data_param[case][\"files_names\"][\"namefile_evt_skim\"]\n",
    "namefile_gen_skim = data_param[case][\"files_names\"][\"namefile_gen_skim\"]\n",
    "var_evt_match = data_param[case][\"variables\"][\"var_evt_match\"]\n",
    "skimming_sel = data_param[case][\"skimming2_sel\"]\n",
    "skimming_sel_gen = data_param[case][\"skimming2_sel_gen\"]\n",
    "skimming_sel_evt = data_param[case][\"skimming2_sel_evt\"]\n",
    "presel_reco = data_param[case][\"presel_reco\"]\n",
    "sel_cent = data_param[case][\"sel_cent\"]\n",
    "skimming2_dotrackpid = data_param[case][\"skimming2_dotrackpid\"]\n",
    "inputdir = data_param[case][\"output_folders\"][\"pkl_out\"][mcordata][indexp]\n",
    "outputdir = data_param[case][\"output_folders\"][\"pkl_skimmed\"][mcordata][indexp]\n",
    "maxfiles = data_config[\"skimming\"][mcordata][\"maxfiles\"]\n",
    "nmaxconvers = data_config[\"skimming\"][mcordata][\"nmaxconvers\"]\n",
    "\n",
    "listfilespath, listfilespathevt, listfilespathgen, \\\n",
    "listfilespathout, listfilespathoutevt, listfilespathoutgen = \\\n",
    "    list_create_dir(inputdir, outputdir, namefile_reco, namefile_evt, namefile_gen, \\\n",
    "                    namefile_reco_skim, namefile_evt_skim, namefile_gen_skim, maxfiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, chunksout = createchunks(listfilespath, listfilespathout, nmaxconvers)\n",
    "chunksevt, chunksoutevt = createchunks(listfilespathevt, listfilespathoutevt, nmaxconvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, _ in enumerate(chunks):\n",
    "#     print(\"Processing chunk number=\", index)\n",
    "#     skimall(chunks[index], chunksevt[index], chunksout[index],\n",
    "#             skimming_sel, var_evt_match, param_case, presel_reco, sel_cent,\n",
    "#             skimming2_dotrackpid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def newskimmer(filein, skimming_sel):\n",
    "    df = pickle.load(open(filein, \"rb\"))\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelizer(function, argument_list, maxperchunk = 2):\n",
    "    chunks = [argument_list[x:x+maxperchunk]  for x in range(0, len(argument_list), maxperchunk)]\n",
    "    for chunk in chunks:\n",
    "        print(chunk)\n",
    "        pool = mp.Pool(10)\n",
    "        [pool.apply(function,args=chunk[i]) for i in range(len(chunk))] \n",
    "        pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentl = [(\"files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl\", skimming_sel), (\"files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl\", skimming_sel), (\"files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl\", skimming_sel), (\"files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl\", skimming_sel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl', 'pt_cand>4 & is_ev_rej==0'), ('files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl', 'pt_cand>4 & is_ev_rej==0')]\n",
      "DONE\n",
      "DONE\n",
      "[('files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl', 'pt_cand>4 & is_ev_rej==0'), ('files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl', 'pt_cand>4 & is_ev_rej==0')]\n",
      "DONE\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "parallelizer(newskimmer, argumentl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def newskimmermap(filein, sel):\n",
    "#     df = pickle.load(open(filein, \"rb\"))\n",
    "#     print(\"DONE\")\n",
    "    \n",
    "# def parallelizer_map(function, argument_list):\n",
    "#     pool = mp.Pool(10)\n",
    "#     pool.map(function, argument_list)\n",
    "#     pool.close()\n",
    "# argumentlmap = [(\"files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl\", \"1\"), (\"files/child_1/0033/AnalysisResultsRecoLctopK0s.pkl\", \"2\")]\n",
    "# parallelizer_map(newskimmermap, argumentlmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
